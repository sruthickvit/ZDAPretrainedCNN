# -*- coding: utf-8 -*-
"""DoS-T-DNN-Generatliation_SruthiWork.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aTcXIO_6jQ5F7_ekEYoj187UFa6I6XFf
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib as plt
import os

from google.colab import drive
drive.mount('/content/drive')

train=pd.read_csv("/content/drive/MyDrive/Sruthi_CK_Research Work/iomtdosnorm.csv",skipinitialspace=True, low_memory=False)

import numpy as np
train.replace([np.inf, -np.inf], np.nan, inplace=True)
train=train.dropna()

train.drop(['label'], axis=1, inplace=True)

import pandas as pd
from sklearn.preprocessing import LabelEncoder

categorical_cols = train.select_dtypes(include=['object']).columns

# Apply Label Encoding to each categorical column
for col in categorical_cols:
    le = LabelEncoder()
    train[col] = le.fit_transform(train[col])

features=[
'Variance', 'ack_flag_number', 'Header_Length',
 'Protocol Type','Std',
'AVG','Tot size', 'Tot sum',
'Rate', 'Covariance',
'Duration','HTTP', 'SSH', 'IRC', 'SMTP', 'Telnet']

y=train['attack']
# Convert categorical labels to numerical using LabelEncoder

#y_train=y_train.astype(float)

X=train[features]

from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

from imblearn.over_sampling import SMOTE
def balance_dataset(X, y):
    smote = SMOTE(random_state=42)
    X_balanced, y_balanced = smote.fit_resample(X, y)

    return X_balanced, y_balanced
(X_train,y_train)=balance_dataset(X_train, y_train)

X_train = pd.DataFrame(X_train)  # Convert to DataFrame

from PIL import Image
import os
import csv

labels = ["Normal", "MITM"]
os.chdir("/content/sample_data")
# Check if the directory exists, if not create it
if not os.path.exists("download1"):
    os.makedirs("download1")

# Check if the directory exists, if not create it
if not os.path.exists("download1"):
    os.makedirs("download1")

# Change the current working directory
os.chdir("download1")

# Convert the mitmds array to uint8
X_train = X_train.astype(np.uint8)

# Iterate over each element in the mitmds array
for i in range(X_train.shape[0]):
    if y_train.iloc[i]==0:
        binary_values = np.unpackbits(X_train.iloc[i].values.view(np.uint8))

    # Group the bits into 8-bit chunks
        grouped_values = np.packbits(binary_values.reshape(-1, 8))

    # Determine the dimensions of the image (assuming a square image)
        image_size = int(np.ceil(np.sqrt(len(grouped_values))))

    # Pad the grouped values with zeros to make a square image
        padded_values = np.pad(grouped_values, (0, image_size**2 - len(grouped_values)), 'constant')

    # Reshape the padded values into a 2D image array
        image_data = padded_values.reshape(image_size, image_size)

    # Create an image from the array and save it
        image = Image.fromarray(image_data, mode='L')
        image.save(f'Normal.{i}.jpg',"JPEG")

labels = ["Attack", "MITM"]
os.chdir("/content/sample_data")
# Check if the directory exists, if not create it
if not os.path.exists("download1"):
    os.makedirs("download1")

# Change the current working directory
os.chdir("download1")


# Convert the mitmds array to uint8
X_train = X_train.astype(np.uint8)

# Iterate over each element in the mitmds array
for i in range(X_train.shape[0]):
    if y_train.iloc[i]==1:
        binary_values = np.unpackbits(X_train.iloc[i].values.view(np.uint8))

    # Group the bits into 8-bit chunks
        grouped_values = np.packbits(binary_values.reshape(-1, 8))

    # Determine the dimensions of the image (assuming a square image)
        image_size = int(np.ceil(np.sqrt(len(grouped_values))))

    # Pad the grouped values with zeros to make a square image
        padded_values = np.pad(grouped_values, (0, image_size**2 - len(grouped_values)), 'constant')

    # Reshape the padded values into a 2D image array
        image_data = padded_values.reshape(image_size, image_size)

    # Create an image from the array and save it
        image = Image.fromarray(image_data, mode='L')
        image.save(f'Attack.{i}.jpg',"JPEG")

labels = ["Normal", "MITM"]
os.chdir("/content/sample_data/")
# Check if the directory exists, if not create it
if not os.path.exists("download1val"):
    os.makedirs("download1val")

# Change the current working directory
os.chdir("download1val")


# Convert the mitmds array to uint8
X_val = X_val.astype(np.uint8)

# Iterate over each element in the mitmds array
for i in range(X_val.shape[0]):
    if y_val.iloc[i]==0:
        binary_values = np.unpackbits(X_val.iloc[i].values.view(np.uint8))

    # Group the bits into 8-bit chunks
        grouped_values = np.packbits(binary_values.reshape(-1, 8))

    # Determine the dimensions of the image (assuming a square image)
        image_size = int(np.ceil(np.sqrt(len(grouped_values))))

    # Pad the grouped values with zeros to make a square image
        padded_values = np.pad(grouped_values, (0, image_size**2 - len(grouped_values)), 'constant')

    # Reshape the padded values into a 2D image array
        image_data = padded_values.reshape(image_size, image_size)

    # Create an image from the array and save it
        image = Image.fromarray(image_data, mode='L')
        image.save(f'Normal.{i}.jpg',"JPEG")
    elif y_val.iloc[i]==1: # Add condition for Attack class
        binary_values = np.unpackbits(X_val.iloc[i].values.view(np.uint8))

        # Group the bits into 8-bit chunks
        grouped_values = np.packbits(binary_values.reshape(-1, 8))

        # Determine the dimensions of the image (assuming a square image)
        image_size = int(np.ceil(np.sqrt(len(grouped_values))))

        # Pad the grouped values with zeros to make a square image
        padded_values = np.pad(grouped_values, (0, image_size**2 - len(grouped_values)), 'constant')

        # Reshape the padded values into a 2D image array
        image_data = padded_values.reshape(image_size, image_size)

        # Create an image from the array and save it
        image = Image.fromarray(image_data, mode='L')
        image.save(f'Attack.{i}.jpg',"JPEG")

os.chdir("/content/sample_data/")

train_img_dir_n = "/content/sample_data/download1"
#train_img_dir_n =train_img_dir_n [:1000]
train_img_paths_n = [os.path.join(train_img_dir_n,filename) for filename in os.listdir(train_img_dir_n)]

import re

train_path_df = pd.DataFrame({
    'path': [],
    'target': []
})

for path in train_img_paths_n:
    pattern = r'Normal'

    match = re.search(pattern, path)

    if match:
        train_path_df = pd.concat([train_path_df, pd.DataFrame({'path': [path], 'target': [0]})], ignore_index=True)
    else:
        train_path_df = pd.concat([train_path_df, pd.DataFrame({'path': [path], 'target': [1]})], ignore_index=True)

val_img_dir_n = "/content/sample_data/download1val"
#train_img_dir_n =train_img_dir_n [:1000]
val_img_paths_n = [os.path.join(val_img_dir_n,filename) for filename in os.listdir(val_img_dir_n)]

import re

val_path_df = pd.DataFrame({
    'path': [],
    'target': []
})

for path in val_img_paths_n:
    pattern = r'Normal'

    match = re.search(pattern, path)

    if match:
        val_path_df = pd.concat([val_path_df, pd.DataFrame({'path': [path], 'target': [0]})], ignore_index=True)
    else:
        val_path_df = pd.concat([val_path_df, pd.DataFrame({'path': [path], 'target': [1]})], ignore_index=True)

from keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator(
    rescale=1./255,
    #validation_split = .2

)

train_path_df['target'] = train_path_df['target'].astype(str)
train_image_generator = datagen.flow_from_dataframe(
    train_path_df,
    x_col='path',
    y_col='target',
    target_size=(75,75),  # Adjust to match your model's input size
   # batch_size=32,
    class_mode='binary',  # Change to 'binary' if you have binary classes
    shuffle=True,
    color_mode='rgb',
    #subset='training'
)

val_path_df['target'] = val_path_df['target'].astype(str)
val_image_generator = datagen.flow_from_dataframe(
    val_path_df,
    x_col='path',
    y_col='target',
    target_size=(75,75),  # Adjust to match your model's input size
    #batch_size=32,
    class_mode='binary',  # Change to 'binary' if you have binary classes
    shuffle=True,
    color_mode='rgb',
    #subset='validation'
)

import os

val_img_dir = "/content/sample_data/download1val"
normal_count = 0
attack_count = 0

if os.path.exists(val_img_dir):
    for filename in os.listdir(val_img_dir):
        if "Normal" in filename:
            normal_count += 1
        elif "Attack" in filename:
            attack_count += 1

print(f"Number of Normal images in {val_img_dir}: {normal_count}")
print(f"Number of Attack images in {val_img_dir}: {attack_count}")

from keras.applications.inception_v3 import InceptionV3
import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop,Adam
from tensorflow.keras import layers
from tensorflow.keras import regularizers
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

inception = InceptionV3(input_shape=(75,75,3), weights='imagenet', include_top=False)
for layer in inception.layers:
    layer.trainable = False

# Define a custom Keras Layer for Feature Dropout
class FeatureDropoutLayer(layers.Layer):
    def __init__(self, drop_prob=0.6, **kwargs):
        super(FeatureDropoutLayer, self).__init__(**kwargs)
        self.drop_prob = drop_prob

    def call(self, inputs, training=None):
        if training:
            mask = tf.cast(tf.random.uniform(tf.shape(inputs)) > self.drop_prob, tf.float32)
            return inputs * mask
        return inputs

x = layers.Flatten()(inception.output)
x = layers.GaussianNoise(0.1)(x)
#x = layers.BatchNormalization()(x)
# Add the custom Feature Dropout layer
x = FeatureDropoutLayer(drop_prob=0.6)(x)
            # normalize before final output
x = layers.Dense(
    256,
    activation='relu',
    kernel_regularizer=regularizers.l2(1e-3),       # penalize large weights
    activity_regularizer=regularizers.l1(1e-3)      # penalize large activations (feature-level reg)
)(x)
x = layers.Dense(
    128,                            # Bottleneck dimension
    activation='relu',
    kernel_regularizer=regularizers.l2(1e-4),
    name="bottleneck_layer"
)(x)




# Add a final sigmoid layer with 1 node for classification output
x = layers.Dense(1, activation='sigmoid')(x)

model = tf.keras.models.Model(inception.input, x)

early_stopping = EarlyStopping(monitor="val_loss", mode="min", verbose=1, patience=10)
lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, mode="min", verbose=1, min_lr=0.001)
call_backs = [ early_stopping, lr_reduce]

from datetime import datetime
start_time = datetime.now()
import keras
model.compile(loss='binary_crossentropy',
              optimizer=keras.optimizers.Adam(learning_rate=0.001),
              metrics=['accuracy'])
"""
batch_size=32
STEP_SIZE_TRAIN = len(train_image_generator) // batch_size
#STEP_SIZE_TRAIN=train_image_generator.n//train_image_generator.batch_size
STEP_SIZE_VALID=len(val_image_generator) // batch_size
STEP_SIZE_TEST=test_image_generator.n//test_image_generator.batch_size
"""
history=model.fit(train_image_generator,
                    #steps_per_epoch=128,
                    validation_data=val_image_generator,
                    #validation_steps=128,
                    epochs=1,
                  callbacks=call_backs
)
#Train the model
#history=model.fit(train_image_generator,epochs=100,  callbacks=call_backs)

end_time = datetime.now()

# Calculate elapsed time
elapsed_time = end_time - start_time

# Print the elapsed time
print(f"Training time: {elapsed_time}")
# Evaluate the model

import pandas as pd
import numpy as np

SOURCE_FEATURE_ORDER = [
    'Variance', 'ack_flag_number', 'psh_flag_number', 'rst_count',
    'Header_Length', 'Magnitue', 'TCP', 'UDP', 'Max', 'Protocol Type',
    'Std', 'Radius', 'AVG', 'Tot size', 'Tot sum', 'syn_count', 'HTTPS',
    'Srate', 'Rate', 'Covariance', 'Min', 'LLC', 'IPv', 'ARP', 'Duration',
    'DNS', 'fin_flag_number', 'syn_flag_number', 'ack_count',
    'rst_flag_number', 'fin_count', 'HTTP', 'IRC', 'SSH', 'SMTP', 'Telnet'
]

# Mapping dictionary: Key is the source feature, Value is the best equivalent in the 82-column list.
# Uses the exact column names from your provided list.
FEATURE_MAPPING = {
    # Statistical / Timing
    'Variance': 'flow_iat.std_SQUARED', # Calculated: Variance is Std Dev squared
    'Std': 'flow_iat.std',
    'Rate': 'flow_pkts_per_sec',
    'Srate': 'fwd_pkts_per_sec',       # Srate (Source Rate) is Fwd Rate
    'Duration': 'flow_duration',

    # TCP Flag Counts (Mapping to flow-level counts)
    'ack_flag_number': 'flow_ACK_flag_count',
    'ack_count': 'flow_ACK_flag_count', # Redundant
    'psh_flag_number': 'fwd_PSH_flag_count', # Using Fwd PSH as proxy for PSH flow count
    'rst_count': 'flow_RST_flag_count',
    'rst_flag_number': 'flow_RST_flag_count', # Redundant
    'syn_count': 'flow_SYN_flag_count',
    'syn_flag_number': 'flow_SYN_flag_count', # Redundant
    'fin_count': 'flow_FIN_flag_count',
    'fin_flag_number': 'flow_FIN_flag_count', # Redundant

    # Size/Length/Bytes
    'Header_Length': 'fwd_header_size_tot', # Using Fwd total header size as proxy
    'Max': 'fwd_pkts_payload.max',      # Using Fwd max payload as proxy
    'Min': 'fwd_pkts_payload.min',      # Using Fwd min payload as proxy
    'Tot size': 'flow_pkts_payload.tot',  # Total payload bytes
    'Tot sum': 'flow_pkts_payload.tot',   # Redundant
    'AVG': 'CALCULATE_OVERALL_MEAN',    # Needs calculation

    # Categorical Features (mapped to Zeek names)
    'Protocol Type': 'proto',
    'TCP': 'PROTO_TCP',
    'UDP': 'PROTO_UDP',
    'IPv': 'PROTO_IP',
    'ARP': 'PROTO_ARP',
    'HTTP': 'SERVICE_http',
    'HTTPS': 'SERVICE_ssl',            # Zeek/Bro often uses 'ssl' for HTTPS
    'DNS': 'SERVICE_dns',
    'SSH': 'SERVICE_ssh',
    'SMTP': 'SERVICE_smtp',
    'IRC': 'SERVICE_irc',
    'Telnet': 'SERVICE_telnet',

    # Features to DROP (no clear equivalent) - These will be set to 0
    'Magnitue': 'SET_TO_ZERO',
    'Radius': 'SET_TO_ZERO',
    'Covariance': 'SET_TO_ZERO',
    'LLC': 'SET_TO_ZERO',
}

def create_transfer_dataframe(target_df):
    """
    Transforms the target DataFrame (82 columns) to match the feature list and order
    of the source model (36 columns).
    """
    df = target_df.copy()

    # --- STEP 1: CALCULATE DERIVED & MAPPED FEATURES ---

    # 1.1 Calculate 'flow_iat.std_SQUARED' for 'Variance'
    # Variance is the square of the Standard Deviation
    # Ensure 'flow_iat.std' exists and handle potential NaNs before squaring
    if 'flow_iat.std' in df.columns:
        df['flow_iat.std_SQUARED'] = df['flow_iat.std'].fillna(0) ** 2
    else:
        df['flow_iat.std_SQUARED'] = 0 # Create the column and fill with 0 if source is missing

    # 1.2 Calculate the overall flow mean payload ('AVG')
    # Using 'flow_pkts_payload.tot' (Total Payload) and total packets (fwd + bwd)
    # Ensure required columns exist and handle potential NaNs/division by zero
    if 'fwd_pkts_tot' in df.columns and 'bwd_pkts_tot' in df.columns and 'flow_pkts_payload.tot' in df.columns:
        df['Total_Flow_Packets'] = df['fwd_pkts_tot'].fillna(0) + df['bwd_pkts_tot'].fillna(0)
        # Calculate Overall Mean Payload (Total Payload / Total Packets)
        # Adding a small epsilon to avoid division by zero
        df['OVERALL_PAYLOAD_MEAN'] = df['flow_pkts_payload.tot'].fillna(0) / (df['Total_Flow_Packets'].replace(0, np.nan) + 1e-6)
        df['OVERALL_PAYLOAD_MEAN'] = df['OVERALL_PAYLOAD_MEAN'].fillna(0)
    else:
        df['OVERALL_PAYLOAD_MEAN'] = 0 # Create the column and fill with 0 if source is missing


    # --- STEP 2: ONE-HOT ENCODING FOR CATEGORICAL FEATURES ---

    # Creating One-Hot columns for the 'proto' and 'service' fields.
    # Note: These exact service names ('http', 'dns', etc.) must exist in the 'service' column.

    # Protocol Encoding (Based on typical Zeek/Bro names)
    if 'proto' in df.columns:
        df['PROTO_TCP'] = np.where(df['proto'].astype(str).str.lower() == 'tcp', 1, 0)
        df['PROTO_UDP'] = np.where(df['proto'].astype(str).str.lower() == 'udp', 1, 0)
        df['PROTO_ARP'] = np.where(df['proto'].astype(str).str.lower() == 'arp', 1, 0)
        df['PROTO_IP'] = np.where(df['proto'].astype(str).str.lower().isin(['ip', 'ipv4', 'ipv6']), 1, 0)
    else:
        # Create columns and fill with 0 if source is missing
        df['PROTO_TCP'] = 0
        df['PROTO_UDP'] = 0
        df['PROTO_ARP'] = 0
        df['PROTO_IP'] = 0


    # Service Encoding (Assuming standard service names)
    if 'service' in df.columns:
        df['SERVICE_http'] = np.where(df['service'].astype(str).str.lower() == 'http', 1, 0)
        df['SERVICE_ssl'] = np.where(df['service'].astype(str).str.lower() == 'ssl', 1, 0)
        df['SERVICE_dns'] = np.where(df['service'].astype(str).str.lower() == 'dns', 1, 0)
        df['SERVICE_ssh'] = np.where(df['service'].astype(str).str.lower() == 'ssh', 1, 0)
        df['SERVICE_smtp'] = np.where(df['service'].astype(str).str.lower() == 'smtp', 1, 0)
        df['SERVICE_irc'] = np.where(df['service'].astype(str).str.lower() == 'irc', 1, 0)
        df['SERVICE_telnet'] = np.where(df['service'].astype(str).str.lower() == 'telnet', 1, 0)
    else:
        # Create columns and fill with 0 if source is missing
        df['SERVICE_http'] = 0
        df['SERVICE_ssl'] = 0
        df['SERVICE_dns'] = 0
        df['SERVICE_ssh'] = 0
        df['SERVICE_smtp'] = 0
        df['SERVICE_irc'] = 0
        df['SERVICE_telnet'] = 0


    # --- STEP 3: CONSTRUCT THE FINAL MAPPED DATAFRAME IN ORDER ---

    final_transfer_df = pd.DataFrame(index=df.index)

    for source_name in SOURCE_FEATURE_ORDER:
        target_name = FEATURE_MAPPING.get(source_name)

        if target_name == 'SET_TO_ZERO':
            final_transfer_df[source_name] = 0 # Set these specific source features to 0
        elif source_name == 'AVG':
            final_transfer_df[source_name] = df['OVERALL_PAYLOAD_MEAN']
        elif source_name == 'Variance':
             final_transfer_df[source_name] = df['flow_iat.std_SQUARED'] # Use the calculated squared std dev

        # Handle Direct Mappings and One-Hot Encoded Columns
        elif target_name in df.columns:
            # Ensure the target column exists in the original or derived DataFrame
            final_transfer_df[source_name] = df[target_name]
        else:
            # If a mapped column is expected but not found (e.g., missing due to data), fill with 0
            print(f"Warning: Mapped target column '{target_name}' for source feature '{source_name}' not found. Filling with 0.")
            final_transfer_df[source_name] = 0


    return final_transfer_df

target_df_mock = pd.read_csv("/content/drive/MyDrive/Sruthi_CK_Research Work/RT_IOT2022Arp_norm.csv",skipinitialspace=True, low_memory=False )

# Call the mapping function
mapped_df = create_transfer_dataframe(target_df_mock)

print("--- Final Mapped DataFrame for Model Transfer ---")
print(f"Original features: {len(SOURCE_FEATURE_ORDER)}")
print(f"Mapped features: {mapped_df.shape[1]}")
print("\nCOLUMNS IN SOURCE ORDER:")
print(mapped_df.columns.tolist())
print("\nDATA (First 5 Rows):")
print(mapped_df.head())

import numpy as np
mapped_df.replace([np.inf, -np.inf], np.nan, inplace=True)
mapped_df=mapped_df.dropna()

#train.drop(['label'], axis=1, inplace=True)
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from PIL import Image
import os


# Identify categorical columns
categorical_cols = mapped_df.select_dtypes(include=['object']).columns

# Apply Label Encoding to each categorical column
for col in categorical_cols:
    le = LabelEncoder()
    mapped_df[col] = le.fit_transform(mapped_df[col])

y_test=target_df_mock['Attack_Type']
# Convert categorical labels to numerical using LabelEncoder

#y_train=y_train.astype(float)

X_test=mapped_df
#X_train=train.drop('attack', axis=1)


from imblearn.over_sampling import SMOTE
def balance_dataset(X, y):
    smote = SMOTE(random_state=42)
    X_balanced, y_balanced = smote.fit_resample(X, y)

    return X_balanced, y_balanced

# Convert X_test to a NumPy array before SMOTE
X_test = X_test.to_numpy()

(X_test,y_test)=balance_dataset(X_test, y_test)

# Add inspection code here
i_to_inspect = 7750
if i_to_inspect < X_test.shape[0]:
    print(f"Inspecting X_test[{i_to_inspect}, :] before astype:")
    print(f"Type: {type(X_test)}")
    print(f"Shape: {X_test.shape}")
    # Check if X_test is a DataFrame or numpy array and access accordingly
    if isinstance(X_test, pd.DataFrame):
        print(f"Content: {X_test.iloc[i_to_inspect, :]}")
    else:
         print(f"Content: {X_test[i_to_inspect, :]}")

else:
    print(f"Index {i_to_inspect} is out of bounds for X_test with shape {X_test.shape} before astype")

os.chdir("/content/sample_data")
# Check if the directory exists, if not create it
if not os.path.exists("download2"):
    os.makedirs("download2")

# Change the current working directory
os.chdir("download2")


# Convert the mitmds array to uint8 and reassign
X_test = X_test.astype(np.uint8)

# Add code to inspect X_test[7750, :]
i_to_inspect = 7750
if i_to_inspect < X_test.shape[0]:
    print(f"Inspecting X_test[{i_to_inspect}, :] after astype:")
    print(f"Type: {type(X_test)}")
    print(f"Shape: {X_test.shape}")
    print(f"Content: {X_test[i_to_inspect, :]}") # Use standard NumPy indexing
else:
    print(f"Index {i_to_inspect} is out of bounds for X_test with shape {X_test.shape}")

# Iterate over each element in the mitmds array
for i in range(X_test.shape[0]):
    if y_test[i]==0:
        binary_values = np.unpackbits(X_test[i, :].view(np.uint8))

    # Group the bits into 8-bit chunks
        grouped_values = np.packbits(binary_values.reshape(-1, 8))

    # Determine the dimensions of the image (assuming a square image)
        image_size = int(np.ceil(np.sqrt(len(grouped_values))))

    # Pad the grouped values with zeros to make a square image
        padded_values = np.pad(grouped_values, (0, image_size**2 - len(grouped_values)), 'constant')

    # Reshape the padded values into a 2D image array
        image_data = padded_values.reshape(image_size, image_size)

    # Create an image from the array and save it
        image = Image.fromarray(image_data, mode='L')
        image.save(f'Normal.{i}.jpg',"JPEG")

os.chdir("/content/sample_data")
# Check if the directory exists, if not create it
if not os.path.exists("download2"):
    os.makedirs("download2")

# Change the current working directory
os.chdir("download2")


# Convert the mitmds array to uint8
X_test = X_test.astype(np.uint8)

# Iterate over each element in the mitmds array
for i in range(X_test.shape[0]):
    if y_test[i]==1:
        binary_values = np.unpackbits(X_test[i, :].view(np.uint8))

    # Group the bits into 8-bit chunks
        grouped_values = np.packbits(binary_values.reshape(-1, 8))

    # Determine the dimensions of the image (assuming a square image)
        image_size = int(np.ceil(np.sqrt(len(grouped_values))))

    # Pad the grouped values with zeros to make a square image
        padded_values = np.pad(grouped_values, (0, image_size**2 - len(grouped_values)), 'constant')

    # Reshape the padded values into a 2D image array
        image_data = padded_values.reshape(image_size, image_size)

    # Create an image from the array and save it
        image = Image.fromarray(image_data, mode='L')
        image.save(f'Attack.{i}.jpg',"JPEG")

test_img_dir_n = "/content/sample_data"
#train_img_dir_n =train_img_dir_n [:1000]
test_img_paths_n = [os.path.join(test_img_dir_n,filename) for filename in os.listdir(test_img_dir_n)]

os.chdir("/content/sample_data")
# Check if the directory exists, if not create it
if not os.path.exists("download2"):
    os.makedirs("download2")

# Change the current working directory
os.chdir("download2")


# Convert the mitmds array to uint8
X_test = X_test.astype(np.uint8)

# Iterate over each element in the mitmds array
for i in range(X_test.shape[0]):
    if y_test[i]==1:
        binary_values = np.unpackbits(X_test[i, :].view(np.uint8))

    # Group the bits into 8-bit chunks
        grouped_values = np.packbits(binary_values.reshape(-1, 8))

    # Determine the dimensions of the image (assuming a square image)
        image_size = int(np.ceil(np.sqrt(len(grouped_values))))

    # Pad the grouped values with zeros to make a square image
        padded_values = np.pad(grouped_values, (0, image_size**2 - len(grouped_values)), 'constant')

    # Reshape the padded values into a 2D image array
        image_data = padded_values.reshape(image_size, image_size)

    # Create an image from the array and save it
        image = Image.fromarray(image_data, mode='L')
        image.save(f'Attack.{i}.jpg',"JPEG")

test_img_dir_n = "/content/sample_data"
#train_img_dir_n =train_img_dir_n [:1000]
test_img_paths_n = [os.path.join(test_img_dir_n,filename) for filename in os.listdir(test_img_dir_n)]

import re
import os

# Define the directory where test images are saved
test_img_dir = "/content/sample_data/download2"

test_path_df = pd.DataFrame({
    'path': [],
    'target': []
})

# List all files in the test image directory
test_img_filenames = os.listdir(test_img_dir)

for filename in test_img_filenames:
    path = os.path.join(test_img_dir, filename)
    pattern = r'Normal'

    match = re.search(pattern, filename) # Search in filename, not full path

    if match:
        test_path_df = pd.concat([test_path_df, pd.DataFrame({'path': [path], 'target': [0]})], ignore_index=True)
    else:
        test_path_df = pd.concat([test_path_df, pd.DataFrame({'path': [path], 'target': [1]})], ignore_index=True)

test_path_df['target'] = test_path_df['target'].astype(str)

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Assuming datagen is defined in a previous cell
# datagen = ImageDataGenerator(rescale=1./255)

test_image_generator = datagen.flow_from_dataframe(
    test_path_df,
    x_col='path',
    y_col='target',
    target_size=(75,75),  # Adjust to match your model's input size
    #batch_size=32,
    class_mode='binary',  # Change to 'binary' if you have binary classes
    shuffle=False,
    color_mode='rgb'
)

import numpy as np
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
#checkpoint_filepath = 'c:/users/lenovo/rtiotdosddos2_12_24.weights.h5'
#model.load_weights(checkpoint_filepath)
predictions = model.predict(test_image_generator)

best_accuracy = 0
best_threshold = 0
best_report = ""

# Calculate ROC AUC directly
true_labels = test_image_generator.classes  # Get true labels once outside the loop
roc_auc = roc_auc_score(true_labels, predictions)  # Use roc_auc_score

for threshold in np.arange(0.1, 1.0, 0.1):
    predicted_labels = (predictions > threshold).astype(int)

    accuracy = accuracy_score(true_labels, predicted_labels)

    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_threshold = threshold
        best_report = classification_report(true_labels, predicted_labels)
        cm = confusion_matrix(true_labels, predicted_labels)

print(f"Best Accuracy: {best_accuracy} at Threshold: {best_threshold}")
print(f"ROC AUC: {roc_auc}")  # Print ROC AUC
print("Best Classification Report:\n", best_report)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()